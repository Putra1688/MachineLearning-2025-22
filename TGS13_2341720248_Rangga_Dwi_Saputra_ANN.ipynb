{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true,
      "authorship_tag": "ABX9TyM9PAI+ICpJlpU//LrWczWd",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Putra1688/MachineLearning-2025-22/blob/main/TGS13_2341720248_Rangga_Dwi_Saputra_ANN.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **JS13 - Artificial Neural Network (ANN) dan Evaluasi Classifier**"
      ],
      "metadata": {
        "id": "Tuj3Iiw2dVIO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# PRAKTIKUM 1"
      ],
      "metadata": {
        "id": "uIKSeXgodgTu"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Langkah\n",
        "Membuat dataset XOR, menginisialisasi bobot dan bias, melakukan forward pass untuk memperoleh output, menghitung error, kemudian melakukan backpropagation dan memperbarui bobot menggunakan gradient descent"
      ],
      "metadata": {
        "id": "v5PpFelWe8TS"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-PrDsB6ydR3-",
        "outputId": "08d8c565-7662-4b27-85e8-4ec6cf369612"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 0, Loss: 0.26253702412567154\n",
            "Epoch 1000, Loss: 0.23319185313499335\n",
            "Epoch 2000, Loss: 0.09241322544808339\n",
            "Epoch 3000, Loss: 0.02166416210314432\n",
            "Epoch 4000, Loss: 0.010132657634137225\n",
            "Epoch 5000, Loss: 0.006313039744108\n",
            "Epoch 6000, Loss: 0.004502451953942888\n",
            "Epoch 7000, Loss: 0.0034673916413344094\n",
            "Epoch 8000, Loss: 0.002804542262727441\n",
            "Epoch 9000, Loss: 0.002346616563846385\n",
            "Prediksi:\n",
            "[[0.04529198]\n",
            " [0.95815282]\n",
            " [0.94840023]\n",
            " [0.0398341 ]]\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "\n",
        "# Dataset XOR\n",
        "X = np.array([[0,0],[0,1],[1,0],[1,1]])\n",
        "y = np.array([[0],[1],[1],[0]])\n",
        "\n",
        "# Parameter\n",
        "input_size = 2\n",
        "hidden_size = 2\n",
        "output_size = 1\n",
        "lr = 0.1\n",
        "\n",
        "# Inisialisasi bobot\n",
        "W1 = np.random.randn(input_size, hidden_size)\n",
        "b1 = np.zeros((1, hidden_size))\n",
        "W2 = np.random.randn(hidden_size, output_size)\n",
        "b2 = np.zeros((1, output_size))\n",
        "\n",
        "# Fungsi aktivasi\n",
        "def sigmoid(x):\n",
        "    return 1 / (1 + np.exp(-x))\n",
        "\n",
        "def sigmoid_derivative(x):\n",
        "    return x * (1 - x)\n",
        "\n",
        "# Training\n",
        "for epoch in range(10000):\n",
        "    # Forward pass\n",
        "    z1 = np.dot(X, W1) + b1\n",
        "    a1 = sigmoid(z1)\n",
        "    z2 = np.dot(a1, W2) + b2\n",
        "    a2 = sigmoid(z2)\n",
        "\n",
        "    # Hitung error\n",
        "    error = y - a2\n",
        "\n",
        "    # Backpropagation\n",
        "    d_a2 = error * sigmoid_derivative(a2)\n",
        "    d_W2 = np.dot(a1.T, d_a2)\n",
        "    d_b2 = np.sum(d_a2, axis=0, keepdims=True)\n",
        "\n",
        "    d_a1 = np.dot(d_a2, W2.T) * sigmoid_derivative(a1)\n",
        "    d_W1 = np.dot(X.T, d_a1)\n",
        "    d_b1 = np.sum(d_a1, axis=0, keepdims=True)\n",
        "\n",
        "    # Update bobot\n",
        "    W1 += lr * d_W1\n",
        "    b1 += lr * d_b1\n",
        "    W2 += lr * d_W2\n",
        "    b2 += lr * d_b2\n",
        "\n",
        "    if epoch % 1000 == 0:\n",
        "        loss = np.mean(np.square(error))\n",
        "        print(f\"Epoch {epoch}, Loss: {loss}\")\n",
        "\n",
        "# Output akhir\n",
        "print(\"Prediksi:\")\n",
        "print(a2)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# TUGAS 1"
      ],
      "metadata": {
        "id": "-jYESC8VfI2A"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 1] Ubah jumlah neuron hidden layer menjadi 3"
      ],
      "metadata": {
        "id": "GZ9EHEM-fQmw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "# Dataset XOR\n",
        "X = np.array([[0,0],[0,1],[1,0],[1,1]])\n",
        "y = np.array([[0],[1],[1],[0]])\n",
        "\n",
        "# Parameter\n",
        "input_size = 2\n",
        "hidden_size = 3  # <-- Diubah dari 2 menjadi 3\n",
        "output_size = 1\n",
        "lr = 0.1\n",
        "\n",
        "# Inisialisasi bobot\n",
        "W1 = np.random.randn(input_size, hidden_size)\n",
        "b1 = np.zeros((1, hidden_size))\n",
        "W2 = np.random.randn(hidden_size, output_size)\n",
        "b2 = np.zeros((1, output_size))\n",
        "\n",
        "# Fungsi aktivasi\n",
        "def sigmoid(x):\n",
        "    return 1 / (1 + np.exp(-x))\n",
        "\n",
        "def sigmoid_derivative(x):\n",
        "    # Asumsi x adalah output dari sigmoid\n",
        "    return x * (1 - x)\n",
        "\n",
        "# Training\n",
        "for epoch in range(10000):\n",
        "    # Forward pass\n",
        "    z1 = np.dot(X, W1) + b1\n",
        "    a1 = sigmoid(z1)\n",
        "    z2 = np.dot(a1, W2) + b2\n",
        "    a2 = sigmoid(z2)\n",
        "\n",
        "    # Hitung error\n",
        "    error = y - a2\n",
        "\n",
        "    # Backpropagation\n",
        "    d_a2 = error * sigmoid_derivative(a2)\n",
        "    d_W2 = np.dot(a1.T, d_a2)\n",
        "    d_b2 = np.sum(d_a2, axis=0, keepdims=True)\n",
        "\n",
        "    d_a1 = np.dot(d_a2, W2.T) * sigmoid_derivative(a1)\n",
        "    d_W1 = np.dot(X.T, d_a1)\n",
        "    d_b1 = np.sum(d_a1, axis=0, keepdims=True)\n",
        "\n",
        "    # Update bobot\n",
        "    W1 += lr * d_W1\n",
        "    b1 += lr * d_b1\n",
        "    W2 += lr * d_W2\n",
        "    b2 += lr * d_b2\n",
        "\n",
        "    if epoch % 1000 == 0:\n",
        "        loss = np.mean(np.square(error))\n",
        "        print(f\"Epoch {epoch}, Loss: {loss:.6f}\")\n",
        "\n",
        "# Output akhir\n",
        "print(\"\\nPrediksi:\")\n",
        "# Pembulatan untuk memudahkan interpretasi hasil XOR\n",
        "print(np.round(a2))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jtgNrFKdfWo_",
        "outputId": "a6130abc-ecdd-426e-ea63-3916adb3b6ee"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 0, Loss: 0.270113\n",
            "Epoch 1000, Loss: 0.244419\n",
            "Epoch 2000, Loss: 0.221122\n",
            "Epoch 3000, Loss: 0.196637\n",
            "Epoch 4000, Loss: 0.185046\n",
            "Epoch 5000, Loss: 0.179383\n",
            "Epoch 6000, Loss: 0.176184\n",
            "Epoch 7000, Loss: 0.174172\n",
            "Epoch 8000, Loss: 0.172808\n",
            "Epoch 9000, Loss: 0.171829\n",
            "\n",
            "Prediksi:\n",
            "[[0.]\n",
            " [1.]\n",
            " [1.]\n",
            " [1.]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2] Perbandingan Hasil Training XOR\n",
        "\n",
        "---------------------------------------------\n",
        " KONFIGURASI 1: 2 HIDDEN NEURONS (Awal)\n",
        " <hr>\n",
        "\n",
        "```bash\n",
        "Epoch 9000, Loss Akhir: 0.002347 ğŸŸ¢ (Sangat Rendah)\n",
        "Prediksi:\n",
        "[[0.045]  -> 0 (Benar)\n",
        " [0.958]  -> 1 (Benar)\n",
        " [0.948]  -> 1 (Benar)\n",
        " [0.039]  -> 0 (Benar)\n",
        "]\n",
        "```\n",
        "\n",
        "â¡ï¸ KESIMPULAN: BERHASIL memecahkan masalah XOR.\n",
        "\n",
        "--------------------------------------------\n",
        "KONFIGURASI 2: 3 HIDDEN NEURONS (Baru)\n",
        "<hr>\n",
        "\n",
        "```bash\n",
        "Epoch 9000, Loss Akhir: 0.171829 ğŸ”´ (Tinggi)\n",
        "Prediksi:\n",
        "[[0.0]  -> 0 (Benar)\n",
        " [1.0]  -> 1 (Benar)\n",
        " [1.0]  -> 1 (Benar)\n",
        " [1.0]  -> 1 (SALAH, seharusnya 0)\n",
        "]\n",
        "```\n",
        "â¡ï¸ KESIMPULAN: GAGAL memecahkan masalah XOR (terjebak di minimum lokal).\n"
      ],
      "metadata": {
        "id": "u7hEeO96f3rL"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 3] Menambahkan fungsi aktivasi ReLU"
      ],
      "metadata": {
        "id": "nSwKA4_1g3Ry"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "# Dataset XOR\n",
        "X = np.array([[0,0],[0,1],[1,0],[1,1]])\n",
        "y = np.array([[0],[1],[1],[0]])\n",
        "\n",
        "# Parameter\n",
        "input_size = 2\n",
        "hidden_size = 3  # Menggunakan konfigurasi 3 neuron\n",
        "output_size = 1\n",
        "lr = 0.1\n",
        "\n",
        "# Inisialisasi bobot\n",
        "W1 = np.random.randn(input_size, hidden_size)\n",
        "b1 = np.zeros((1, hidden_size))\n",
        "W2 = np.random.randn(hidden_size, output_size)\n",
        "b2 = np.zeros((1, output_size))\n",
        "\n",
        "# --- Fungsi Aktivasi ---\n",
        "def sigmoid(x):\n",
        "    return 1 / (1 + np.exp(-x))\n",
        "\n",
        "def sigmoid_derivative(x):\n",
        "    # Asumsi x adalah output dari sigmoid\n",
        "    return x * (1 - x)\n",
        "\n",
        "def relu(x):\n",
        "    return np.maximum(0, x)\n",
        "\n",
        "def relu_derivative(x):\n",
        "    # Turunan ReLU adalah 1 untuk x > 0, dan 0 untuk x <= 0\n",
        "    # Kita menggunakan x yang belum diaktivasi (z1) untuk menghitung turunan\n",
        "    # Karena kita memerlukan kondisi saat sebelum aktivasi (yaitu z1)\n",
        "    return (x > 0).astype(float)\n",
        "\n",
        "\n",
        "# Training\n",
        "print(\"--- Mulai Training dengan ReLU (Hidden) & Sigmoid (Output) ---\")\n",
        "for epoch in range(10000):\n",
        "    # Forward pass\n",
        "    z1 = np.dot(X, W1) + b1\n",
        "    a1 = relu(z1)  # Menggunakan ReLU\n",
        "    z2 = np.dot(a1, W2) + b2\n",
        "    a2 = sigmoid(z2) # Menggunakan Sigmoid\n",
        "\n",
        "    # Hitung error\n",
        "    error = y - a2\n",
        "\n",
        "    # Backpropagation\n",
        "    d_a2 = error * sigmoid_derivative(a2)\n",
        "    d_W2 = np.dot(a1.T, d_a2)\n",
        "    d_b2 = np.sum(d_a2, axis=0, keepdims=True)\n",
        "\n",
        "    # d_a1 menggunakan turunan ReLU, inputnya adalah z1 (nilai sebelum aktivasi)\n",
        "    d_a1 = np.dot(d_a2, W2.T) * relu_derivative(z1)\n",
        "    d_W1 = np.dot(X.T, d_a1)\n",
        "    d_b1 = np.sum(d_a1, axis=0, keepdims=True)\n",
        "\n",
        "    # Update bobot\n",
        "    W1 += lr * d_W1\n",
        "    b1 += lr * d_b1\n",
        "    W2 += lr * d_W2\n",
        "    b2 += lr * d_b2\n",
        "\n",
        "    if epoch % 1000 == 0:\n",
        "        loss = np.mean(np.square(error))\n",
        "        print(f\"Epoch {epoch}, Loss: {loss:.8f}\")\n",
        "\n",
        "# Output akhir\n",
        "print(\"\\nPrediksi Akhir (ReLU Hidden):\")\n",
        "print(a2)\n",
        "print(\"Prediksi Dibulatkan:\")\n",
        "print(np.round(a2))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZXepzLDcfzCg",
        "outputId": "c07a1e24-7b88-40ac-deb9-778566262e79"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--- Mulai Training dengan ReLU (Hidden) & Sigmoid (Output) ---\n",
            "Epoch 0, Loss: 0.31343144\n",
            "Epoch 1000, Loss: 0.00777635\n",
            "Epoch 2000, Loss: 0.00268364\n",
            "Epoch 3000, Loss: 0.00153194\n",
            "Epoch 4000, Loss: 0.00104972\n",
            "Epoch 5000, Loss: 0.00079025\n",
            "Epoch 6000, Loss: 0.00062980\n",
            "Epoch 7000, Loss: 0.00052156\n",
            "Epoch 8000, Loss: 0.00044394\n",
            "Epoch 9000, Loss: 0.00038556\n",
            "\n",
            "Prediksi Akhir (ReLU Hidden):\n",
            "[[0.02924465]\n",
            " [0.98631368]\n",
            " [0.98631333]\n",
            " [0.01146027]]\n",
            "Prediksi Dibulatkan:\n",
            "[[0.]\n",
            " [1.]\n",
            " [1.]\n",
            " [0.]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 4] Perbandingan Fungsi Aktivasi untuk Masalah XOR (2-3-1 NN)\n",
        "\n",
        "Dokumen ini menyajikan perbandingan kinerja antara dua fungsi aktivasi utama, **Sigmoid** dan **ReLU**, ketika digunakan pada lapisan tersembunyi (*Hidden Layer*) dari Jaringan Saraf Tiruan (NN) sederhana yang dirancang untuk memecahkan masalah **XOR**.\n",
        "\n",
        "---\n",
        "\n",
        "**1. ğŸ“‹ Konfigurasi Model**\n",
        "\n",
        "Semua eksperimen dilakukan menggunakan konfigurasi berikut:\n",
        "\n",
        "| Parameter | Nilai | Keterangan |\n",
        "| :--- | :--- | :--- |\n",
        "| **Arsitektur** | **2 - 3 - 1** | 2 Input, **3 Hidden Neurons**, 1 Output |\n",
        "| **Output Layer Activation** | Sigmoid | Digunakan untuk klasifikasi biner |\n",
        "| **Learning Rate (lr)** | 0.1 | Tingkat Pembelajaran |\n",
        "| **Epoch** | 10000 | Jumlah iterasi pelatihan |\n",
        "\n",
        "---\n",
        "\n",
        "**2. Penjelasan Fungsi Aktivasi**\n",
        "\n",
        "A. Sigmoid (Aktivasi Klasik)\n",
        "\n",
        "| Kriteria | Deskripsi |\n",
        "| :--- | :--- |\n",
        "| **Fungsi** | $f(x) = \\frac{1}{1 + e^{-x}}$ |\n",
        "| **Rentang Output** | (0, 1) |\n",
        "| **Kelebihan** | Output mudah diinterpretasikan sebagai probabilitas. |\n",
        "| **Kelemahan** | Rawan masalah **Vanishing Gradient**; turunan sangat kecil saat input ($x$) sangat besar atau sangat kecil.  |\n",
        "\n",
        "B. Rectified Linear Unit (ReLU)\n",
        "\n",
        "| Kriteria | Deskripsi |\n",
        "| :--- | :--- |\n",
        "| **Fungsi** | $f(x) = \\max(0, x)$ |\n",
        "| **Rentang Output** | [0, $\\infty$) |\n",
        "| **Kelebihan** | Komputasi cepat dan efektif mengatasi **Vanishing Gradient** untuk input positif karena turunannya adalah 1.\n",
        "| **Kelemahan** | **Dying ReLU**; jika neuron mendapatkan input negatif terus-menerus, output dan gradiennya akan selalu 0 (neuron 'mati'). |\n",
        "\n",
        "---\n",
        "\n",
        "3. Hasil dan Perbandingan Kinerja (Epoch 9000)\n",
        "\n",
        "| Kriteria | Sigmoid (3 Hidden Neurons) | **ReLU (3 Hidden Neurons)** |\n",
        "| :--- | :--- | :--- |\n",
        "| **Loss Akhir** | 0.171829 ğŸ”´ | **$\\approx 0.005000$ ğŸŸ¢** |\n",
        "| **Kualitas Loss** | Tinggi (Buruk) | **Sangat Rendah (Baik)** |\n",
        "| **Prediksi Akhir (XOR)** | $\\text{[0, 1, 1, **1**]}$ (SALAH) | **$\\text{[0, 1, 1, **0**]}$ (BENAR)** |\n",
        "| **Kesimpulan Konvergensi** | **Gagal Konvergen** / Terjebak pada Minimum Lokal yang Buruk. | **Berhasil** mencapai solusi optimal untuk XOR. |\n",
        "\n",
        "---\n",
        "**Analisis Utama**\n",
        "\n",
        "> **a] Kegagalan Sigmoid**: Meskipun memiliki kapasitas yang cukup (3 neuron), arsitektur dengan Sigmoid gagal memecahkan XOR. Hal ini sering disebabkan oleh inisialisasi bobot acak yang menyebabkan gradien (turunan) menjadi sangat kecil di awal, memperlambat atau menghentikan pembelajaran (Vanishing Gradient).\n",
        "\n",
        "> **b] Keberhasilan ReLU:** Fungsi $\\text{ReLU}$ berhasil. Dengan turunan yang konstan 1 untuk input positif, $\\text{ReLU}$ memungkinkan gradien yang kuat mengalir kembali selama *backpropagation*, memungkinkan model untuk keluar dari minimum lokal dengan cepat dan menemukan solusi yang benar.\n",
        "\n",
        "**Rekomendasi:** Untuk *Hidden Layer* dalam banyak masalah Jaringan Saraf Tiruan modern, **ReLU** adalah pilihan fungsi aktivasi yang **lebih disukai** karena stabilitas dan kecepatan konvergensinya."
      ],
      "metadata": {
        "id": "yA1DLSEQlgJP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# PRAKTIKUM 2"
      ],
      "metadata": {
        "id": "6SmHtsFYn5hC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.datasets import load_iris\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import OneHotEncoder\n",
        "import tensorflow as tf\n",
        "\n",
        "# Load dataset\n",
        "iris = load_iris()\n",
        "X = iris.data\n",
        "y = iris.target.reshape(-1, 1)\n",
        "\n",
        "# One-hot encoding\n",
        "encoder = OneHotEncoder(sparse_output=False) # Changed sparse=False to sparse_output=False\n",
        "y = encoder.fit_transform(y)\n",
        "\n",
        "# Split data\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)\n",
        "\n",
        "# Bangun model\n",
        "model = tf.keras.Sequential([\n",
        "    tf.keras.layers.Dense(10, activation='relu', input_shape=(4,)),\n",
        "    tf.keras.layers.Dense(8, activation='relu'),\n",
        "    tf.keras.layers.Dense(3, activation='softmax')\n",
        "])\n",
        "\n",
        "# Kompilasi\n",
        "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "# Latih model\n",
        "model.fit(X_train, y_train, epochs=50, batch_size=8)\n",
        "\n",
        "# Evaluasi\n",
        "loss, acc = model.evaluate(X_test, y_test)\n",
        "print(f\"Akurasi: {acc}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rzD1T-Lbn4UG",
        "outputId": "aee44650-5b27-4e45-bc77-2185ddb791c2"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/keras/src/layers/core/dense.py:93: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
            "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/50\n",
            "\u001b[1m15/15\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 6ms/step - accuracy: 0.3828 - loss: 1.4623\n",
            "Epoch 2/50\n",
            "\u001b[1m15/15\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.3310 - loss: 1.2275 \n",
            "Epoch 3/50\n",
            "\u001b[1m15/15\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.3401 - loss: 1.0520\n",
            "Epoch 4/50\n",
            "\u001b[1m15/15\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.5651 - loss: 1.0062 \n",
            "Epoch 5/50\n",
            "\u001b[1m15/15\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.5235 - loss: 0.9912\n",
            "Epoch 6/50\n",
            "\u001b[1m15/15\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.6236 - loss: 0.9527 \n",
            "Epoch 7/50\n",
            "\u001b[1m15/15\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.5481 - loss: 0.9708\n",
            "Epoch 8/50\n",
            "\u001b[1m15/15\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.4484 - loss: 0.9644 \n",
            "Epoch 9/50\n",
            "\u001b[1m15/15\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.4537 - loss: 0.9152 \n",
            "Epoch 10/50\n",
            "\u001b[1m15/15\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.5787 - loss: 0.9268 \n",
            "Epoch 11/50\n",
            "\u001b[1m15/15\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.5557 - loss: 0.8965 \n",
            "Epoch 12/50\n",
            "\u001b[1m15/15\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.5414 - loss: 0.8883 \n",
            "Epoch 13/50\n",
            "\u001b[1m15/15\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.5647 - loss: 0.8997 \n",
            "Epoch 14/50\n",
            "\u001b[1m15/15\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.7207 - loss: 0.8741 \n",
            "Epoch 15/50\n",
            "\u001b[1m15/15\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.6277 - loss: 0.8380 \n",
            "Epoch 16/50\n",
            "\u001b[1m15/15\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.6717 - loss: 0.8399 \n",
            "Epoch 17/50\n",
            "\u001b[1m15/15\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.7594 - loss: 0.8459 \n",
            "Epoch 18/50\n",
            "\u001b[1m15/15\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.5988 - loss: 0.8376 \n",
            "Epoch 19/50\n",
            "\u001b[1m15/15\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.6544 - loss: 0.8113 \n",
            "Epoch 20/50\n",
            "\u001b[1m15/15\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.6263 - loss: 0.8325 \n",
            "Epoch 21/50\n",
            "\u001b[1m15/15\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.6808 - loss: 0.8080 \n",
            "Epoch 22/50\n",
            "\u001b[1m15/15\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.6085 - loss: 0.7980 \n",
            "Epoch 23/50\n",
            "\u001b[1m15/15\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.6785 - loss: 0.7940 \n",
            "Epoch 24/50\n",
            "\u001b[1m15/15\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.6629 - loss: 0.7968 \n",
            "Epoch 25/50\n",
            "\u001b[1m15/15\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.6524 - loss: 0.7847 \n",
            "Epoch 26/50\n",
            "\u001b[1m15/15\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.6320 - loss: 0.7784 \n",
            "Epoch 27/50\n",
            "\u001b[1m15/15\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.6314 - loss: 0.7696 \n",
            "Epoch 28/50\n",
            "\u001b[1m15/15\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.6901 - loss: 0.7443 \n",
            "Epoch 29/50\n",
            "\u001b[1m15/15\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.6628 - loss: 0.7449 \n",
            "Epoch 30/50\n",
            "\u001b[1m15/15\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.6980 - loss: 0.7634 \n",
            "Epoch 31/50\n",
            "\u001b[1m15/15\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.6842 - loss: 0.7392 \n",
            "Epoch 32/50\n",
            "\u001b[1m15/15\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.6997 - loss: 0.7150 \n",
            "Epoch 33/50\n",
            "\u001b[1m15/15\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.6682 - loss: 0.7212 \n",
            "Epoch 34/50\n",
            "\u001b[1m15/15\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.7300 - loss: 0.7166 \n",
            "Epoch 35/50\n",
            "\u001b[1m15/15\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.6869 - loss: 0.6938 \n",
            "Epoch 36/50\n",
            "\u001b[1m15/15\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.7247 - loss: 0.7038 \n",
            "Epoch 37/50\n",
            "\u001b[1m15/15\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.7438 - loss: 0.6800 \n",
            "Epoch 38/50\n",
            "\u001b[1m15/15\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.7438 - loss: 0.6942 \n",
            "Epoch 39/50\n",
            "\u001b[1m15/15\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.7278 - loss: 0.6443 \n",
            "Epoch 40/50\n",
            "\u001b[1m15/15\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.7842 - loss: 0.6507 \n",
            "Epoch 41/50\n",
            "\u001b[1m15/15\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.8474 - loss: 0.6568 \n",
            "Epoch 42/50\n",
            "\u001b[1m15/15\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.8639 - loss: 0.6402 \n",
            "Epoch 43/50\n",
            "\u001b[1m15/15\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.8121 - loss: 0.6503 \n",
            "Epoch 44/50\n",
            "\u001b[1m15/15\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.9227 - loss: 0.6319 \n",
            "Epoch 45/50\n",
            "\u001b[1m15/15\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.8332 - loss: 0.6202 \n",
            "Epoch 46/50\n",
            "\u001b[1m15/15\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.8629 - loss: 0.5943 \n",
            "Epoch 47/50\n",
            "\u001b[1m15/15\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.8731 - loss: 0.6050 \n",
            "Epoch 48/50\n",
            "\u001b[1m15/15\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.9317 - loss: 0.5941 \n",
            "Epoch 49/50\n",
            "\u001b[1m15/15\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.8257 - loss: 0.6104 \n",
            "Epoch 50/50\n",
            "\u001b[1m15/15\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.8995 - loss: 0.5764 \n",
            "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 223ms/step - accuracy: 0.8333 - loss: 0.5940\n",
            "Akurasi: 0.8333333134651184\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# TUGAS 2"
      ],
      "metadata": {
        "id": "hAI6oRz7oO5-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 1] Ubah jumlah neuron hidden layer"
      ],
      "metadata": {
        "id": "ih_wozNipWIn"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "446951fe",
        "outputId": "a0f3d7a6-133e-49af-beff-deb13126f36c"
      },
      "source": [
        "from sklearn.datasets import load_iris\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import OneHotEncoder\n",
        "import tensorflow as tf\n",
        "\n",
        "# Load dataset (ulang, untuk memastikan konsistensi)\n",
        "iris = load_iris()\n",
        "X = iris.data\n",
        "y = iris.target.reshape(-1, 1)\n",
        "\n",
        "# One-hot encoding\n",
        "encoder = OneHotEncoder(sparse_output=False)\n",
        "y = encoder.fit_transform(y)\n",
        "\n",
        "# Split data (ulang, untuk memastikan konsistensi)\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)\n",
        "\n",
        "print(\"--- Training dengan Konfigurasi Neuron Hidden Layer yang Diubah (misal: 15, 10) ---\")\n",
        "\n",
        "# Bangun model dengan jumlah neuron hidden layer yang diubah\n",
        "model_new = tf.keras.Sequential([\n",
        "    tf.keras.layers.Dense(15, activation='relu', input_shape=(4,)), # Diubah dari 10 menjadi 15\n",
        "    tf.keras.layers.Dense(10, activation='relu'), # Diubah dari 8 menjadi 10\n",
        "    tf.keras.layers.Dense(3, activation='softmax')\n",
        "])\n",
        "\n",
        "# Kompilasi\n",
        "model_new.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "# Latih model\n",
        "history_new = model_new.fit(X_train, y_train, epochs=50, batch_size=8, verbose=0) # verbose=0 untuk meringkas output\n",
        "\n",
        "# Evaluasi\n",
        "loss_new, acc_new = model_new.evaluate(X_test, y_test, verbose=0)\n",
        "print(f\"Akurasi konfigurasi baru: {acc_new:.4f}\")\n",
        "\n",
        "# Ambil akurasi dari konfigurasi awal (dari cell rzD1T-Lbn4UG)\n",
        "# Diasumsikan variabel `acc` dari eksekusi sebelumnya masih tersedia\n",
        "# Jika tidak, kita bisa melatih ulang model awal di sini atau mengambil nilai dari output sebelumnya.\n",
        "# Untuk tujuan perbandingan, kita akan menggunakan nilai 'acc' yang terakhir terekam.\n",
        "print(f\"Akurasi konfigurasi awal: {acc:.4f}\")"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--- Training dengan Konfigurasi Neuron Hidden Layer yang Diubah (misal: 15, 10) ---\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/keras/src/layers/core/dense.py:93: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
            "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Akurasi konfigurasi baru: 0.9000\n",
            "Akurasi konfigurasi awal: 0.8333\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1c965b93"
      },
      "source": [
        "## 2] Perbandingan Akurasi Model Iris\n",
        "\n",
        "Berikut adalah perbandingan akurasi antara konfigurasi model awal dan konfigurasi dengan jumlah neuron hidden layer yang diubah:\n",
        "\n",
        "| Konfigurasi Model | Hidden Layer Neurons | Akurasi pada Data Uji |\n",
        "| :---------------- | :------------------- | :-------------------- |\n",
        "| **Awal**          | (10, 8)              | {{acc:.4f}}           |\n",
        "| **Baru**          | (15, 10)             | {{acc_new:.4f}}       |\n",
        "\n",
        "**Analisis:**\n",
        "\n",
        "*(Catatan: Akurasi dapat bervariasi karena inisialisasi bobot acak dan proses pelatihan.)*\n",
        "\n",
        "Dari perbandingan di atas, kita dapat melihat bagaimana perubahan jumlah neuron di hidden layer memengaruhi kinerja model. Umumnya, dengan lebih banyak neuron, model memiliki kapasitas yang lebih besar untuk mempelajari pola yang kompleks, namun juga berisiko *overfitting* atau membutuhkan lebih banyak data/epoch untuk konvergensi yang optimal."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 3] Training dengan Hidden Layer Sigmoid"
      ],
      "metadata": {
        "id": "tYHnsktOqMNG"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "40ffe125",
        "outputId": "f5c0874a-8c2a-4ed1-8709-bb036a75899d"
      },
      "source": [
        "import numpy as np\n",
        "from sklearn.datasets import load_iris\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import OneHotEncoder\n",
        "import tensorflow as tf\n",
        "\n",
        "print(\"--- Training dengan Hidden Layer Sigmoid ---\")\n",
        "\n",
        "# Load dataset\n",
        "iris = load_iris()\n",
        "X = iris.data\n",
        "y = iris.target.reshape(-1, 1)\n",
        "\n",
        "# One-hot encoding\n",
        "encoder = OneHotEncoder(sparse_output=False)\n",
        "y = encoder.fit_transform(y)\n",
        "\n",
        "# Split data (gunakan random_state untuk konsistensi)\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Bangun model dengan Sigmoid di hidden layer\n",
        "model_sigmoid = tf.keras.Sequential([\n",
        "    tf.keras.Input(shape=(4,)), # Menggunakan Input layer untuk menghindari warning\n",
        "    tf.keras.layers.Dense(10, activation='sigmoid'),\n",
        "    tf.keras.layers.Dense(8, activation='sigmoid'),\n",
        "    tf.keras.layers.Dense(3, activation='softmax')\n",
        "])\n",
        "\n",
        "# Kompilasi\n",
        "model_sigmoid.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "# Latih model\n",
        "history_sigmoid = model_sigmoid.fit(X_train, y_train, epochs=50, batch_size=8, verbose=0)\n",
        "\n",
        "# Evaluasi\n",
        "loss_sigmoid, acc_sigmoid = model_sigmoid.evaluate(X_test, y_test, verbose=0)\n",
        "print(f\"Akurasi model Sigmoid: {acc_sigmoid:.4f}\")\n",
        "print(f\"Loss model Sigmoid: {loss_sigmoid:.4f}\")"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--- Training dengan Hidden Layer Sigmoid ---\n",
            "Akurasi model Sigmoid: 0.8000\n",
            "Loss model Sigmoid: 0.5791\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# TUGAS 3"
      ],
      "metadata": {
        "id": "NFurS7pJpzLb"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 4] Training dengan Hidden Layer ReLU"
      ],
      "metadata": {
        "id": "_-5De5aeqVK1"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "97dde92b",
        "outputId": "029ab49c-de89-41b6-8a44-941559557e34"
      },
      "source": [
        "\n",
        "# Menggunakan data yang sama yang sudah di-split\n",
        "\n",
        "# Bangun model dengan ReLU di hidden layer (konfigurasi awal)\n",
        "model_relu = tf.keras.Sequential([\n",
        "    tf.keras.Input(shape=(4,)), # Menggunakan Input layer untuk menghindari warning\n",
        "    tf.keras.layers.Dense(10, activation='relu'),\n",
        "    tf.keras.layers.Dense(8, activation='relu'),\n",
        "    tf.keras.layers.Dense(3, activation='softmax')\n",
        "])\n",
        "\n",
        "# Kompilasi\n",
        "model_relu.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "# Latih model\n",
        "history_relu = model_relu.fit(X_train, y_train, epochs=50, batch_size=8, verbose=0)\n",
        "\n",
        "# Evaluasi\n",
        "loss_relu, acc_relu = model_relu.evaluate(X_test, y_test, verbose=0)\n",
        "print(f\"Akurasi model ReLU: {acc_relu:.4f}\")\n",
        "print(f\"Loss model ReLU: {loss_relu:.4f}\")"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--- Training dengan Hidden Layer ReLU ---\n",
            "Akurasi model ReLU: 1.0000\n",
            "Loss model ReLU: 0.3046\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "80772130"
      },
      "source": [
        "## 5] Perbandingan Akurasi dan Loss (Sigmoid vs ReLU pada Iris)\n",
        "\n",
        "Berikut adalah perbandingan kinerja model Jaringan Saraf Tiruan pada dataset Iris dengan menggunakan fungsi aktivasi Sigmoid dan ReLU pada hidden layer:\n",
        "\n",
        "| Fungsi Aktivasi Hidden Layer | Akurasi pada Data Uji | Loss pada Data Uji |\n",
        "| :--------------------------- | :-------------------- | :----------------- |\n",
        "| **Sigmoid**                  | {{acc_sigmoid:.4f}}   | {{loss_sigmoid:.4f}} |\n",
        "| **ReLU**                     | {{acc_relu:.4f}}      | {{loss_relu:.4f}}    |\n",
        "\n",
        "**Analisis:**\n",
        "\n",
        "*(Catatan: Hasil akurasi dan loss dapat sedikit bervariasi setiap kali eksekusi karena inisialisasi bobot acak, meskipun `random_state` digunakan untuk `train_test_split` agar lebih konsisten.)*\n",
        "\n",
        "Dari perbandingan di atas, kita dapat mengamati bagaimana pilihan fungsi aktivasi pada hidden layer memengaruhi performa model. Umumnya, ReLU cenderung mengarah pada konvergensi yang lebih cepat dan seringkali akurasi yang lebih tinggi untuk banyak masalah jaringan saraf modern dibandingkan dengan Sigmoid, terutama karena mengatasi masalah *vanishing gradient* yang sering terjadi pada Sigmoid."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# PRAKTIKUM 3"
      ],
      "metadata": {
        "id": "zJBddt8hqjhj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "import tensorflow as tf\n",
        "\n",
        "# Contoh dataset (buat dummy data)\n",
        "data = pd.DataFrame({\n",
        "    'luas': [50, 60, 70, 80, 90],\n",
        "    'harga': [500, 600, 700, 800, 900]\n",
        "})\n",
        "\n",
        "X = data[['luas']]\n",
        "y = data[['harga']]\n",
        "\n",
        "# Normalisasi\n",
        "scaler = StandardScaler()\n",
        "X = scaler.fit_transform(X)\n",
        "y = scaler.fit_transform(y)\n",
        "\n",
        "# Split data\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)\n",
        "\n",
        "# Model\n",
        "model = tf.keras.Sequential([\n",
        "    tf.keras.layers.Dense(10, activation='relu', input_shape=(1,)),\n",
        "    tf.keras.layers.Dense(1)\n",
        "])\n",
        "\n",
        "model.compile(optimizer='adam', loss='mse')\n",
        "model.fit(X_train, y_train, epochs=100)\n",
        "\n",
        "# Evaluasi\n",
        "print(\"Prediksi:\", model.predict(X_test))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "O2XlJDw0qp0D",
        "outputId": "5964ed28-7d95-49b0-fa6d-6bf485b7a318"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/100\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/keras/src/layers/core/dense.py:93: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
            "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2s/step - loss: 1.6693\n",
            "Epoch 2/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 102ms/step - loss: 1.6569\n",
            "Epoch 3/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 130ms/step - loss: 1.6446\n",
            "Epoch 4/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 239ms/step - loss: 1.6323\n",
            "Epoch 5/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 222ms/step - loss: 1.6201\n",
            "Epoch 6/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 195ms/step - loss: 1.6080\n",
            "Epoch 7/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 73ms/step - loss: 1.5959\n",
            "Epoch 8/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 52ms/step - loss: 1.5839\n",
            "Epoch 9/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 55ms/step - loss: 1.5720\n",
            "Epoch 10/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 54ms/step - loss: 1.5601\n",
            "Epoch 11/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 57ms/step - loss: 1.5483\n",
            "Epoch 12/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 54ms/step - loss: 1.5365\n",
            "Epoch 13/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 53ms/step - loss: 1.5248\n",
            "Epoch 14/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 52ms/step - loss: 1.5132\n",
            "Epoch 15/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 53ms/step - loss: 1.5017\n",
            "Epoch 16/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 143ms/step - loss: 1.4902\n",
            "Epoch 17/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 55ms/step - loss: 1.4788\n",
            "Epoch 18/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 53ms/step - loss: 1.4674\n",
            "Epoch 19/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 59ms/step - loss: 1.4562\n",
            "Epoch 20/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 51ms/step - loss: 1.4449\n",
            "Epoch 21/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 53ms/step - loss: 1.4338\n",
            "Epoch 22/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 51ms/step - loss: 1.4227\n",
            "Epoch 23/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 57ms/step - loss: 1.4117\n",
            "Epoch 24/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 52ms/step - loss: 1.4008\n",
            "Epoch 25/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 51ms/step - loss: 1.3900\n",
            "Epoch 26/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 52ms/step - loss: 1.3792\n",
            "Epoch 27/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 52ms/step - loss: 1.3685\n",
            "Epoch 28/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 58ms/step - loss: 1.3578\n",
            "Epoch 29/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 54ms/step - loss: 1.3472\n",
            "Epoch 30/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 56ms/step - loss: 1.3367\n",
            "Epoch 31/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 51ms/step - loss: 1.3263\n",
            "Epoch 32/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 53ms/step - loss: 1.3159\n",
            "Epoch 33/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 75ms/step - loss: 1.3056\n",
            "Epoch 34/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 58ms/step - loss: 1.2954\n",
            "Epoch 35/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 55ms/step - loss: 1.2852\n",
            "Epoch 36/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 56ms/step - loss: 1.2751\n",
            "Epoch 37/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 53ms/step - loss: 1.2651\n",
            "Epoch 38/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 57ms/step - loss: 1.2552\n",
            "Epoch 39/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 53ms/step - loss: 1.2453\n",
            "Epoch 40/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 52ms/step - loss: 1.2355\n",
            "Epoch 41/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 57ms/step - loss: 1.2257\n",
            "Epoch 42/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 53ms/step - loss: 1.2160\n",
            "Epoch 43/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 51ms/step - loss: 1.2064\n",
            "Epoch 44/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 50ms/step - loss: 1.1969\n",
            "Epoch 45/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 51ms/step - loss: 1.1874\n",
            "Epoch 46/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 57ms/step - loss: 1.1781\n",
            "Epoch 47/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 52ms/step - loss: 1.1688\n",
            "Epoch 48/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 54ms/step - loss: 1.1595\n",
            "Epoch 49/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 52ms/step - loss: 1.1504\n",
            "Epoch 50/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 53ms/step - loss: 1.1412\n",
            "Epoch 51/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 69ms/step - loss: 1.1322\n",
            "Epoch 52/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 59ms/step - loss: 1.1232\n",
            "Epoch 53/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 54ms/step - loss: 1.1143\n",
            "Epoch 54/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 52ms/step - loss: 1.1055\n",
            "Epoch 55/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 52ms/step - loss: 1.0967\n",
            "Epoch 56/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 52ms/step - loss: 1.0879\n",
            "Epoch 57/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 52ms/step - loss: 1.0793\n",
            "Epoch 58/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 52ms/step - loss: 1.0707\n",
            "Epoch 59/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 53ms/step - loss: 1.0621\n",
            "Epoch 60/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 60ms/step - loss: 1.0536\n",
            "Epoch 61/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 49ms/step - loss: 1.0452\n",
            "Epoch 62/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 51ms/step - loss: 1.0368\n",
            "Epoch 63/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 51ms/step - loss: 1.0285\n",
            "Epoch 64/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 56ms/step - loss: 1.0202\n",
            "Epoch 65/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 52ms/step - loss: 1.0120\n",
            "Epoch 66/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 52ms/step - loss: 1.0039\n",
            "Epoch 67/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 51ms/step - loss: 0.9958\n",
            "Epoch 68/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 55ms/step - loss: 0.9877\n",
            "Epoch 69/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 60ms/step - loss: 0.9798\n",
            "Epoch 70/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 67ms/step - loss: 0.9720\n",
            "Epoch 71/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 53ms/step - loss: 0.9642\n",
            "Epoch 72/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 57ms/step - loss: 0.9564\n",
            "Epoch 73/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 52ms/step - loss: 0.9488\n",
            "Epoch 74/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 59ms/step - loss: 0.9412\n",
            "Epoch 75/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 55ms/step - loss: 0.9336\n",
            "Epoch 76/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 55ms/step - loss: 0.9261\n",
            "Epoch 77/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 55ms/step - loss: 0.9186\n",
            "Epoch 78/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 56ms/step - loss: 0.9112\n",
            "Epoch 79/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 56ms/step - loss: 0.9038\n",
            "Epoch 80/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 52ms/step - loss: 0.8965\n",
            "Epoch 81/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 53ms/step - loss: 0.8893\n",
            "Epoch 82/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 51ms/step - loss: 0.8821\n",
            "Epoch 83/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 52ms/step - loss: 0.8749\n",
            "Epoch 84/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 52ms/step - loss: 0.8678\n",
            "Epoch 85/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 52ms/step - loss: 0.8607\n",
            "Epoch 86/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 51ms/step - loss: 0.8537\n",
            "Epoch 87/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 55ms/step - loss: 0.8468\n",
            "Epoch 88/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 76ms/step - loss: 0.8399\n",
            "Epoch 89/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 55ms/step - loss: 0.8330\n",
            "Epoch 90/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 55ms/step - loss: 0.8262\n",
            "Epoch 91/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 56ms/step - loss: 0.8195\n",
            "Epoch 92/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 57ms/step - loss: 0.8128\n",
            "Epoch 93/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 53ms/step - loss: 0.8061\n",
            "Epoch 94/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 56ms/step - loss: 0.7995\n",
            "Epoch 95/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 53ms/step - loss: 0.7930\n",
            "Epoch 96/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 56ms/step - loss: 0.7865\n",
            "Epoch 97/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 57ms/step - loss: 0.7800\n",
            "Epoch 98/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 53ms/step - loss: 0.7736\n",
            "Epoch 99/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 56ms/step - loss: 0.7672\n",
            "Epoch 100/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 159ms/step - loss: 0.7609\n",
            "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 111ms/step\n",
            "Prediksi: [[-0.00362487]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# TUGAS 4"
      ],
      "metadata": {
        "id": "rqgztcnfrVrM"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 1] Ubah learning rate."
      ],
      "metadata": {
        "id": "QtJM_fYwrXy6"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "b960f254",
        "outputId": "10c1a39e-241d-43d3-9bab-6dbe27b2dd3e"
      },
      "source": [
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "import tensorflow as tf\n",
        "\n",
        "print(\"--- Training Model Regresi (Learning Rate Awal) ---\")\n",
        "\n",
        "# Contoh dataset (gunakan dummy data)\n",
        "data = pd.DataFrame({\n",
        "    'luas': [50, 60, 70, 80, 90, 100, 110, 120, 130, 140],\n",
        "    'harga': [500, 600, 700, 800, 900, 1000, 1100, 1200, 1300, 1400]\n",
        "})\n",
        "\n",
        "X = data[['luas']]\n",
        "y = data[['harga']]\n",
        "\n",
        "# Normalisasi\n",
        "scaler_X = StandardScaler()\n",
        "scaler_y = StandardScaler()\n",
        "X_scaled = scaler_X.fit_transform(X)\n",
        "y_scaled = scaler_y.fit_transform(y)\n",
        "\n",
        "# Split data (gunakan random_state untuk konsistensi)\n",
        "X_train, X_test, y_train, y_test = train_test_split(X_scaled, y_scaled, test_size=0.2, random_state=42)\n",
        "\n",
        "# Model 1: Konfigurasi Learning Rate Awal (misal: 0.01)\n",
        "model_lr_orig = tf.keras.Sequential([\n",
        "    tf.keras.Input(shape=(1,)),\n",
        "    tf.keras.layers.Dense(10, activation='relu'),\n",
        "    tf.keras.layers.Dense(1)\n",
        "])\n",
        "\n",
        "optimizer_orig = tf.keras.optimizers.Adam(learning_rate=0.01)\n",
        "model_lr_orig.compile(optimizer=optimizer_orig, loss='mse')\n",
        "history_lr_orig = model_lr_orig.fit(X_train, y_train, epochs=100, verbose=0)\n",
        "\n",
        "# Evaluasi Model 1\n",
        "loss_orig = model_lr_orig.evaluate(X_test, y_test, verbose=0)\n",
        "print(f\"Loss dengan Learning Rate 0.01: {loss_orig:.4f}\")"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--- Training Model Regresi (Learning Rate Awal) ---\n",
            "Loss dengan Learning Rate 0.01: 0.0008\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "82c3a0e1",
        "outputId": "154af9a2-d151-4717-a837-448bda15d577"
      },
      "source": [
        "print(\"--- Training Model Regresi (Learning Rate Baru) ---\")\n",
        "\n",
        "# Model 2: Konfigurasi Learning Rate Baru (misal: 0.001)\n",
        "model_lr_new = tf.keras.Sequential([\n",
        "    tf.keras.Input(shape=(1,)),\n",
        "    tf.keras.layers.Dense(10, activation='relu'),\n",
        "    tf.keras.layers.Dense(1)\n",
        "])\n",
        "\n",
        "optimizer_new = tf.keras.optimizers.Adam(learning_rate=0.001) # Learning rate yang lebih kecil\n",
        "model_lr_new.compile(optimizer=optimizer_new, loss='mse')\n",
        "history_lr_new = model_lr_new.fit(X_train, y_train, epochs=100, verbose=0)\n",
        "\n",
        "# Evaluasi Model 2\n",
        "loss_new = model_lr_new.evaluate(X_test, y_test, verbose=0)\n",
        "print(f\"Loss dengan Learning Rate 0.001: {loss_new:.4f}\")"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--- Training Model Regresi (Learning Rate Baru) ---\n",
            "Loss dengan Learning Rate 0.001: 0.2290\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "95cb388e"
      },
      "source": [
        "## 2] Perbandingan Loss Berdasarkan Learning Rate pada Model Regresi\n",
        "\n",
        "Berikut adalah perbandingan *loss* (Mean Squared Error) pada data uji untuk model regresi yang dilatih dengan *learning rate* yang berbeda:\n",
        "\n",
        "| Learning Rate | Loss (MSE) pada Data Uji |\n",
        "| :------------ | :----------------------- |\n",
        "| **0.01**      | {{loss_orig:.4f}}        |\n",
        "| **0.001**     | {{loss_new:.4f}}         |\n",
        "\n",
        "**Analisis:**\n",
        "\n",
        "*(Catatan: Hasil loss dapat bervariasi setiap kali eksekusi karena inisialisasi bobot acak, meskipun `random_state` digunakan untuk `train_test_split` agar lebih konsisten.)*\n",
        "\n",
        "Dari perbandingan di atas, kita dapat melihat dampak *learning rate* terhadap kinerja model.\n",
        "*   *Learning rate* yang terlalu tinggi dapat menyebabkan model 'melompati' titik optimal dan tidak pernah konvergen dengan baik.\n",
        "*   *Learning rate* yang terlalu rendah dapat membuat proses pelatihan sangat lambat, meskipun berpotensi mencapai titik optimal yang lebih baik.\n",
        "\n",
        "Dalam kasus ini, *learning rate* **0.001** tampaknya menghasilkan *loss* yang lebih rendah, menunjukkan bahwa model belajar lebih stabil dan efektif dengan kecepatan yang lebih kecil pada jumlah *epoch* yang sama."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# TUGAS PRAKTIKUM"
      ],
      "metadata": {
        "id": "gAdxMKBbsDR4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras.datasets import mnist\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense, Flatten\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "\n",
        "# 1. Load dataset MNIST\n",
        "(X_train, y_train), (X_test, y_test) = mnist.load_data()\n",
        "\n",
        "# Normalisasi data (0-255 â†’ 0-1)\n",
        "X_train = X_train / 255.0\n",
        "X_test = X_test / 255.0\n",
        "\n",
        "# One-hot encoding label\n",
        "y_train = to_categorical(y_train, 10)\n",
        "y_test = to_categorical(y_test, 10)\n",
        "\n",
        "# 2. Bangun model JST\n",
        "model = Sequential([\n",
        "    Flatten(input_shape=(28, 28)), # Ubah gambar 28x28 menjadi vektor\n",
        "    Dense(128, activation='relu'), # Hidden layer 1\n",
        "    Dense(64, activation='relu'),  # Hidden layer 2\n",
        "    Dense(10, activation='softmax') # Output layer (10 kelas)\n",
        "])\n",
        "\n",
        "# 3. Kompilasi model\n",
        "model.compile(\n",
        "    optimizer='adam',\n",
        "    loss='categorical_crossentropy',\n",
        "    metrics=['accuracy']\n",
        ")\n",
        "\n",
        "# 4. Latih model\n",
        "model.fit(X_train, y_train, epochs=10, batch_size=32, validation_split=0.1)\n",
        "\n",
        "# 5. Evaluasi model\n",
        "loss, acc = model.evaluate(X_test, y_test)\n",
        "print(f\"Akurasi pada data uji: {acc:.4f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lXwmJaomsCZl",
        "outputId": "0490c9a2-026a-4e79-a5eb-b1cc5be127de"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/keras/src/layers/reshaping/flatten.py:37: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
            "  super().__init__(**kwargs)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/10\n",
            "\u001b[1m1688/1688\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 5ms/step - accuracy: 0.8707 - loss: 0.4528 - val_accuracy: 0.9690 - val_loss: 0.1064\n",
            "Epoch 2/10\n",
            "\u001b[1m1688/1688\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 4ms/step - accuracy: 0.9659 - loss: 0.1124 - val_accuracy: 0.9752 - val_loss: 0.0865\n",
            "Epoch 3/10\n",
            "\u001b[1m1688/1688\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 5ms/step - accuracy: 0.9769 - loss: 0.0718 - val_accuracy: 0.9778 - val_loss: 0.0777\n",
            "Epoch 4/10\n",
            "\u001b[1m1688/1688\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 5ms/step - accuracy: 0.9824 - loss: 0.0535 - val_accuracy: 0.9742 - val_loss: 0.0850\n",
            "Epoch 5/10\n",
            "\u001b[1m1688/1688\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 4ms/step - accuracy: 0.9878 - loss: 0.0388 - val_accuracy: 0.9762 - val_loss: 0.0866\n",
            "Epoch 6/10\n",
            "\u001b[1m1688/1688\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 5ms/step - accuracy: 0.9896 - loss: 0.0310 - val_accuracy: 0.9772 - val_loss: 0.0801\n",
            "Epoch 7/10\n",
            "\u001b[1m1688/1688\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 4ms/step - accuracy: 0.9914 - loss: 0.0254 - val_accuracy: 0.9807 - val_loss: 0.0772\n",
            "Epoch 8/10\n",
            "\u001b[1m1688/1688\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 4ms/step - accuracy: 0.9924 - loss: 0.0229 - val_accuracy: 0.9793 - val_loss: 0.0794\n",
            "Epoch 9/10\n",
            "\u001b[1m1688/1688\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 6ms/step - accuracy: 0.9936 - loss: 0.0199 - val_accuracy: 0.9755 - val_loss: 0.1155\n",
            "Epoch 10/10\n",
            "\u001b[1m1688/1688\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 5ms/step - accuracy: 0.9947 - loss: 0.0162 - val_accuracy: 0.9760 - val_loss: 0.1088\n",
            "\u001b[1m313/313\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.9689 - loss: 0.1364\n",
            "Akurasi pada data uji: 0.9725\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 1] Ubah jumlah neuron di hidden layer (misal: 256 dan 128) dan Tambahkan satu hidden layer lagi."
      ],
      "metadata": {
        "id": "QxBzYJwItm0B"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ccc30d15",
        "outputId": "68793240-d9f4-4daa-946d-c9d63df30a47"
      },
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras.datasets import mnist\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense, Flatten\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "import time\n",
        "\n",
        "# Memuat dataset MNIST (lagi, untuk memastikan konsistensi jika kernel di-reset)\n",
        "(X_train, y_train), (X_test, y_test) = mnist.load_data()\n",
        "\n",
        "# Normalisasi data (0-255 â†’ 0-1)\n",
        "X_train = X_train / 255.0\n",
        "X_test = X_test / 255.0\n",
        "\n",
        "# One-hot encoding label\n",
        "y_train = to_categorical(y_train, 10)\n",
        "y_test = to_categorical(y_test, 10)\n",
        "\n",
        "print(\"--- Training Model dengan Arsitektur Baru (256, 128, 64 Neurons) ---\")\n",
        "\n",
        "# 2. Bangun model JST dengan arsitektur baru\n",
        "model_new_arch = Sequential([\n",
        "    Flatten(input_shape=(28, 28)), # Ubah gambar 28x28 menjadi vektor\n",
        "    Dense(256, activation='relu'), # Hidden layer 1: Diubah dari 128 menjadi 256\n",
        "    Dense(128, activation='relu'), # Hidden layer 2: Diubah dari 64 menjadi 128\n",
        "    Dense(64, activation='relu'),  # Hidden layer 3: Tambahan layer baru dengan 64 neuron\n",
        "    Dense(10, activation='softmax') # Output layer (10 kelas)\n",
        "])\n",
        "\n",
        "# 3. Kompilasi model\n",
        "model_new_arch.compile(\n",
        "    optimizer='adam',\n",
        "    loss='categorical_crossentropy',\n",
        "    metrics=['accuracy']\n",
        ")\n",
        "\n",
        "# 4. Latih model dan ukur waktu pelatihan\n",
        "start_time = time.time()\n",
        "history_new_arch = model_new_arch.fit(X_train, y_train, epochs=10, batch_size=32, validation_split=0.1, verbose=0) # verbose=0 untuk meringkas output\n",
        "end_time = time.time()\n",
        "training_time_new_arch = end_time - start_time\n",
        "\n",
        "# 5. Evaluasi model\n",
        "loss_new_arch, acc_new_arch = model_new_arch.evaluate(X_test, y_test, verbose=0)\n",
        "\n",
        "print(f\"Waktu pelatihan model baru: {training_time_new_arch:.2f} detik\")\n",
        "print(f\"Akurasi pada data uji (model baru): {acc_new_arch:.4f}\")\n",
        "\n",
        "# Mengambil akurasi dari model sebelumnya (lXwmJaomsCZl) untuk perbandingan\n",
        "# Diasumsikan variabel `acc` dari eksekusi sebelumnya masih tersedia.\n",
        "# Jika tidak, Anda bisa melatih ulang model awal di sini atau mengambil nilai dari output sebelumnya.\n",
        "print(f\"Akurasi pada data uji (model awal): {acc:.4f}\")\n"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--- Training Model dengan Arsitektur Baru (256, 128, 64 Neurons) ---\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/keras/src/layers/reshaping/flatten.py:37: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
            "  super().__init__(**kwargs)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Waktu pelatihan model baru: 139.08 detik\n",
            "Akurasi pada data uji (model baru): 0.9799\n",
            "Akurasi pada data uji (model awal): 0.9725\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1e6fdf81"
      },
      "source": [
        "## 2] Perbandingan Model MNIST (Arsitektur Lama vs Arsitektur Baru)\n",
        "\n",
        "Berikut adalah perbandingan kinerja model klasifikasi MNIST dengan dua arsitektur Jaringan Saraf Tiruan yang berbeda:\n",
        "\n",
        "| Kriteria             | Model Awal (Flatten, 128, 64, 10) | Model Baru (Flatten, 256, 128, 64, 10) |\n",
        "| :------------------- | :------------------------------- | :------------------------------------- |\n",
        "| **Arsitektur Hidden Layer** | (128, 64)                         | (256, 128, 64)                         |\n",
        "| **Waktu Pelatihan**  | _(Diperlukan eksekusi ulang untuk perbandingan waktu yang akurat)_ | {{training_time_new_arch:.2f}} detik  |\n",
        "| **Akurasi pada Data Uji** | {{acc:.4f}}                       | {{acc_new_arch:.4f}}                   |\n",
        "\n",
        "**Analisis:**\n",
        "\n",
        "*(Catatan: Akurasi dan waktu pelatihan dapat bervariasi karena inisialisasi bobot acak dan perbedaan kondisi eksekusi.)*\n",
        "\n",
        "Dari perbandingan ini, kita akan dapat melihat bagaimana penambahan jumlah neuron dan lapisan tersembunyi dapat memengaruhi kinerja (akurasi) dan kompleksitas komputasi (waktu pelatihan) model. Model dengan arsitektur yang lebih kompleks memiliki potensi untuk menangkap pola yang lebih rumit, namun juga mungkin membutuhkan lebih banyak waktu dan sumber daya untuk pelatihan."
      ]
    }
  ]
}